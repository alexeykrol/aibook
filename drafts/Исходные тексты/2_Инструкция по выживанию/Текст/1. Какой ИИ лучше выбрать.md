1\. Какой ИИ лучше выбрать? 

Разных ИИ очень много. В каких случаях какие модели использовать? С какого начать?

**Короткий ответ:** 

В 95% любых случаев chatGPT от OpenAI лучший выбор, который покрывет почти любые потребности, кроме специальных случаев, о которых речь пойдет ниже. Почему chatGPT? Потому что на данный момент chatGPT версии 5 бьет все другие модели в 90% задач от текста до программирования, математики + на данный момент лучше всего работает с русским языком. Начинайте с  chatGPT, не ошибетесь, а по мере чтения этой книги узнаете другие модели, которые эффективней применять для специфический создать, связанных с программированием, творчеством.

**Полный ответ (для тех, кто хочет глубже):** 

Из основных моделей есть американские chatGPT от OpenAI, Gemini от Google, Claude от Anthropic , Grok от xAI (Илон Маск), китайские DeepSeek, Qwen, европейская [mistral.ai](http://mistral.ai), россиская GigaChat. Однако, если вам мало , можно зайти на HuggingFace — здесь почти 2,5 миллиона моделей на любой вкус, причём многие из них специализированные. Их можно запускать прямо там и экспериментировать сколько угодно (если умеете).

Внутри все модели примерно одинаковые, есть тьма бенчмарков, или лучше сказать — соревнований между моделями (кто умней и искуссней), и в целом все модели показывают +/- примерно одинаковые результаты, периодически выходя в лидеры или уступая кому-то в зависимости от того, какие задачи вы ставите перед ИИ.

Разумеется, отдельно мы поговорим о специализированных моделях для программирования, создания изображений, видео, музыки, обработки аудио, не говоря о том, что можели имеют несколько режимов и уровней способностей, что очень ценно, и обо все этом ниже.

Однако, если взять условно 99% всех людей, которые пользуются искусственным интеллектом, то основная масса запросов в стиле —  «дай мне рецепт горохового супа», и для этих целей chatGPT  — best choice. Поэтому, голову себе ломать не надо, и будем вам счастье. Тем не менее, за последние пол-года все модели подтянулись, и возможно, сейчас, когда вы это читаете уже доступны chatGPT 6, Genini 3 и Grok 5.

Оговорка -  я говорю только о платных тарифах, потому что они дают нормальный уровень возможностей. Я трачу на ИИ около 1000$/ месяц, но у меня много специализированных задач и это самое эффективное расходование средств в моей жизни. Однако, вы можете начать с 20$/месяц, и, поверьте, никогда не пожалеете об этом. 

Важное отличие моделей - размер контекстного окна, грубо говоря - какой объем текста вы можете впихнуть при запросе к модели. 

Сравнительная таблица контекстных окон ИИ-моделей

| Категория | Модель | Размер контекста (токены) | Ключевые особенности |
| :----- | :----- | :----- | :----- |
| Экспериментальные | Magic.dev LTM-2-Mini | 100,000,000 | Теоретически может обработать целые репозитории кода или сотни романов, но практическое применение пока не подтверждено [codingscape](https://codingscape.com/blog/llms-with-largest-context-windows) . |
| Сверхбольшие окна | Meta Llama 4 Scout | 10,000,000 | Высокая производительность на одной GPU, подходит для анализа видео и полной обработки книг [codingscape+1](https://codingscape.com/blog/llms-with-largest-context-windows) . |
| Очень большие окна | Claude Sonnet 4 | 1,000,000 | Доступно в бета-версии через API для анализа больших наборов документов и кодовых баз [codingscape+1](https://codingscape.com/blog/llms-with-largest-context-windows) . |
|  | Google Gemini 2.5 (Pro & Flash) | 1,000,000 | Предназначены для сложных мультимодальных задач и корпоративного анализа документов [codingscape+1](https://codingscape.com/blog/llms-with-largest-context-windows) . |
|  | OpenAI GPT-4.1 | 1,000,000 | Обеспечивает высокую производительность для задач, требующих большого контекста [codingscape+1](https://codingscape.com/blog/llms-with-largest-context-windows) . |
|  | Qwen3-Next | до 1,000,000 | Нативно поддерживает 256,000 токенов с возможностью расширения до 1 миллиона [codingscape](https://codingscape.com/blog/llms-with-largest-context-windows) . |
| Крупные окна | OpenAI GPT-5 | 400,000 | Улучшенные возможности рассуждения и производительность при работе с длинным контекстом [codingscape](https://codingscape.com/blog/llms-with-largest-context-windows) . |
|  | Qwen3-Max-Preview | 258,000 | Модель с более чем триллионом параметров для сложных задач [codingscape](https://codingscape.com/blog/llms-with-largest-context-windows) . |
|  | Anthropic Claude 4 (Opus) | 200,000 | Оптимизирована для высокоточных многоэтапных задач и глубоких исследований [codingscape+1](https://codingscape.com/blog/llms-with-largest-context-windows) . |
|  | Anthropic Claude 3.5 Sonnet | 200,000 | Подходит для большинства бизнес-задач, детального анализа документов и сложного кодирования [eesel](https://www.eesel.ai/blog/claude-code-context-window-size) . |
| Стандартные большие окна | OpenAI GPT-4o | 128,000 | Баланс между эффективностью и производительностью для различных задач, включая анализ изображений [codingscape](https://codingscape.com/blog/llms-with-largest-context-windows) . |
|  | OpenAI GPT-4 Turbo | 128,000 | Позволяет обрабатывать около 300 страниц текста за один запрос [milvus](https://milvus.io/ai-quick-reference/what-is-the-maximum-context-window-for-openais-models) . |
|  | Mistral Large 2 | 128,000 | Гибкое решение для продвинутого обобщения текста и генерации кода [codingscape](https://codingscape.com/blog/llms-with-largest-context-windows) . |
|  | DeepSeek R1 & V3 | 128,000 | Эффективные модели для развертывания на устройствах [codingscape](https://codingscape.com/blog/llms-with-largest-context-windows) . |
| Ранние и базовые модели | Gemini | 32,000 | Стандартная версия для общих задач [dev](https://dev.to/jiminlee/understanding-context-window-size-in-llms-3aof) . |
|  | GPT-4 (стандартный) | 8,000–32,000 | Ранние версии GPT-4 имели меньший объем контекста [milvus+1](https://milvus.io/ai-quick-reference/what-is-the-maximum-context-window-for-openais-models) . |
|  | GPT-3.5 Turbo | 16,000 | Более старая модель с ограниченным контекстом по сравнению с новыми версиями [milvus](https://milvus.io/ai-quick-reference/what-is-the-maximum-context-window-for-openais-models) . |

Для 99% людей, которые заходят на ИИ «за рецептами» — контекстное окно в миллион токенов это очень, очень много. Даже при самом плохом раскладе — это два-три миллиона символов текста. Кто из вас хоть что-то пишет, понимает, насколько это чудовищно много. Средний художественный роман — это примерно полмиллиона символов. 

Контекстное окно в 1 млн. токенов тянет при самом плохом раскладе — на три-четыре полноценных романа. Две «Войны и мир», грубо говоря. Для многих людей на планете это в тысячу раз больше, чем им нужно за всю жизнь.

**Важно!** У вас должна быть заранее готова карта, с которой вы можете платить - позаботьтесь об этом заранее. Также вы можете оплачивать с помощью Apple Pay.